{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook for squeezenet 1.0 and 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "Verify that all dependencies are installed using the cell below. Continue if no errors encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import time, logging\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.data import imagenet\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "\n",
    "import os\n",
    "from mxnet.context import cpu\n",
    "from mxnet.gluon.block import HybridBlock\n",
    "from mxnet.gluon.contrib.nn import HybridConcurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify model, hyperparameters and save locations\n",
    "The values shown below are used to obtain the model in the model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model - squeezenet1.0 or squeezenet1.1\n",
    "model_name = 'squeezenet1.0' \n",
    "# training and validation pictures to use\n",
    "data_dir = '../imagenet/img_dataset'\n",
    "# training batch size per device (CPU/GPU)\n",
    "batch_size = 128\n",
    "# number of GPUs to use\n",
    "num_gpus = 4\n",
    "# number of pre-processing workers\n",
    "num_workers = 32\n",
    "# number of training epochs\n",
    "num_epochs = 1\n",
    "# learning rate\n",
    "lr = 0.01\n",
    "# momentum value for optimizer\n",
    "momentum = 0.9\n",
    "# weight decay rate\n",
    "wd = 0.0002\n",
    "# decay rate of learning rate\n",
    "lr_decay = 0.1\n",
    "# interval for periodic learning rate decays\n",
    "lr_decay_period = 0\n",
    "# epoches at which learning rate decays\n",
    "lr_decay_epoch = '60,90'\n",
    "# mode in which to train the model. options are symbolic, imperative, hybrid\n",
    "mode = 'hybrid'\n",
    "# use label smoothing or not in training\n",
    "label_smoothing = False\n",
    "# continue training a pre-trained model\n",
    "use_pretrained = False\n",
    "# Number of batches to wait before logging\n",
    "log_interval = 100\n",
    "# frequency of model saving\n",
    "save_frequency = 10\n",
    "# directory of saved models\n",
    "save_dir = 'params'\n",
    "#directory of training logs\n",
    "logging_dir = 'logs'\n",
    "# the path to save the history plot\n",
    "save_plot_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition in Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SqueezeNet, implemented in Gluon.\"\"\"\n",
    "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
    "\n",
    "# Helpers\n",
    "def _make_fire(squeeze_channels, expand1x1_channels, expand3x3_channels):\n",
    "    out = nn.HybridSequential(prefix='')\n",
    "    out.add(_make_fire_conv(squeeze_channels, 1))\n",
    "\n",
    "    paths = HybridConcurrent(axis=1, prefix='')\n",
    "    paths.add(_make_fire_conv(expand1x1_channels, 1))\n",
    "    paths.add(_make_fire_conv(expand3x3_channels, 3, 1))\n",
    "    out.add(paths)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _make_fire_conv(channels, kernel_size, padding=0):\n",
    "    out = nn.HybridSequential(prefix='')\n",
    "    out.add(nn.Conv2D(channels, kernel_size, padding=padding))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    return out\n",
    "\n",
    "# Net\n",
    "class SqueezeNet(HybridBlock):\n",
    "    r\"\"\"SqueezeNet model from the `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n",
    "    and <0.5MB model size\" <https://arxiv.org/abs/1602.07360>`_ paper.\n",
    "    SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
    "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
    "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
    "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    version : str\n",
    "        Version of squeezenet. Options are '1.0', '1.1'.\n",
    "    classes : int, default 1000\n",
    "        Number of classification classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, version, classes=1000, **kwargs):\n",
    "        super(SqueezeNet, self).__init__(**kwargs)\n",
    "        assert version in ['1.0', '1.1'], (\"Unsupported SqueezeNet version {version}:\"\n",
    "                                           \"1.0 or 1.1 expected\".format(version=version))\n",
    "        with self.name_scope():\n",
    "            self.features = nn.HybridSequential(prefix='')\n",
    "            if version == '1.0':\n",
    "                self.features.add(nn.Conv2D(96, kernel_size=7, strides=2))\n",
    "                self.features.add(nn.Activation('relu'))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(16, 64, 64))\n",
    "                self.features.add(_make_fire(16, 64, 64))\n",
    "                self.features.add(_make_fire(32, 128, 128))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(32, 128, 128))\n",
    "                self.features.add(_make_fire(48, 192, 192))\n",
    "                self.features.add(_make_fire(48, 192, 192))\n",
    "                self.features.add(_make_fire(64, 256, 256))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(64, 256, 256))\n",
    "            else:\n",
    "                self.features.add(nn.Conv2D(64, kernel_size=3, strides=2))\n",
    "                self.features.add(nn.Activation('relu'))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(16, 64, 64))\n",
    "                self.features.add(_make_fire(16, 64, 64))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(32, 128, 128))\n",
    "                self.features.add(_make_fire(32, 128, 128))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(48, 192, 192))\n",
    "                self.features.add(_make_fire(48, 192, 192))\n",
    "                self.features.add(_make_fire(64, 256, 256))\n",
    "                self.features.add(_make_fire(64, 256, 256))\n",
    "            self.features.add(nn.Dropout(0.5))\n",
    "\n",
    "            self.output = nn.HybridSequential(prefix='')\n",
    "            self.output.add(nn.Conv2D(classes, kernel_size=1))\n",
    "            self.output.add(nn.Activation('relu'))\n",
    "            self.output.add(nn.AvgPool2D(13))\n",
    "            self.output.add(nn.Flatten())\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.features(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Constructor\n",
    "def get_squeezenet(version, pretrained=False, ctx=cpu(),\n",
    "                   root=os.path.join('~', '.mxnet', 'models'), **kwargs):\n",
    "    r\"\"\"SqueezeNet model from the `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n",
    "    and <0.5MB model size\" <https://arxiv.org/abs/1602.07360>`_ paper.\n",
    "    SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
    "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
    "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
    "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    version : str\n",
    "        Version of squeezenet. Options are '1.0', '1.1'.\n",
    "    pretrained : bool, default False\n",
    "        Whether to load the pretrained weights for model.\n",
    "    ctx : Context, default CPU\n",
    "        The context in which to load the pretrained weights.\n",
    "    root : str, default '~/.mxnet/models'\n",
    "        Location for keeping the model parameters.\n",
    "    \"\"\"\n",
    "    net = SqueezeNet(version, **kwargs)\n",
    "    if pretrained:\n",
    "        from ..model_store import get_model_file\n",
    "        net.load_params(get_model_file('squeezenet%s'%version, root=root), ctx=ctx)\n",
    "    return net\n",
    "\n",
    "def squeezenet1_0(**kwargs):\n",
    "    r\"\"\"SqueezeNet 1.0 model from the `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n",
    "    and <0.5MB model size\" <https://arxiv.org/abs/1602.07360>`_ paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained : bool, default False\n",
    "        Whether to load the pretrained weights for model.\n",
    "    ctx : Context, default CPU\n",
    "        The context in which to load the pretrained weights.\n",
    "    root : str, default '~/.mxnet/models'\n",
    "        Location for keeping the model parameters.\n",
    "    \"\"\"\n",
    "    return get_squeezenet('1.0', **kwargs)\n",
    "\n",
    "def squeezenet1_1(**kwargs):\n",
    "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
    "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
    "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
    "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained : bool, default False\n",
    "        Whether to load the pretrained weights for model.\n",
    "    ctx : Context, default CPU\n",
    "        The context in which to load the pretrained weights.\n",
    "    root : str, default '~/.mxnet/models'\n",
    "        Location for keeping the model parameters.\n",
    "    \"\"\"\n",
    "    return get_squeezenet('1.1', **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train, test and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "classes = 1000\n",
    "batch_size *= max(1, num_gpus)\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
    "\n",
    "lr_decay_epoch = [int(i) for i in lr_decay_epoch.split(',')] + [np.inf]\n",
    "\n",
    "kwargs = {'ctx': context, 'pretrained': use_pretrained, 'classes': classes}\n",
    "\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'learning_rate': lr, 'wd': wd, 'momentum': momentum}\n",
    "\n",
    "if model_name == 'squeezenet1.0':\n",
    "    net = squeezenet1_0(**kwargs)\n",
    "else:\n",
    "    net = squeezenet1_1(**kwargs)\n",
    "\n",
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "train_history = TrainingHistory(['training-top1-err', 'training-top5-err',\n",
    "                                 'validation-top1-err', 'validation-top5-err'])\n",
    "\n",
    "save_frequency = save_frequency\n",
    "if save_dir and save_frequency:\n",
    "    save_dir = save_dir\n",
    "    makedirs(save_dir)\n",
    "else:\n",
    "    save_dir = ''\n",
    "    save_frequency = 0\n",
    "\n",
    "plot_path = save_plot_dir\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n",
    "                                 saturation=jitter_param),\n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def smooth(label, classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        ind = l.astype('int')\n",
    "        res = nd.zeros((ind.shape[0], classes), ctx = l.context)\n",
    "        res += eta/classes\n",
    "        res[nd.arange(ind.shape[0], ctx = l.context), ind] = 1 - eta + eta/classes\n",
    "        smoothed.append(res)\n",
    "    return smoothed\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "\n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    return (1-top1, 1-top5)\n",
    "\n",
    "def train(epochs, ctx):\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n",
    "    train_data = gluon.data.DataLoader(\n",
    "        imagenet.classification.ImageNet(data_dir, train=True).transform_first(transform_train),\n",
    "        batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "    val_data = gluon.data.DataLoader(\n",
    "        imagenet.classification.ImageNet(data_dir, train=False).transform_first(transform_test),\n",
    "        batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "    if label_smoothing:\n",
    "        L = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "    else:\n",
    "        L = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    lr_decay_count = 0\n",
    "\n",
    "    best_val_score = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        acc_top1.reset()\n",
    "        acc_top5.reset()\n",
    "        btic = time.time()\n",
    "        train_loss = 0\n",
    "        num_batch = len(train_data)\n",
    "\n",
    "        if lr_decay_period and epoch and epoch % lr_decay_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        elif lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]:\n",
    "            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "            lr_decay_count += 1\n",
    "\n",
    "        for i, batch in enumerate(train_data):\n",
    "            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "            if label_smoothing:\n",
    "                label_smooth = smooth(label, classes)\n",
    "            else:\n",
    "                label_smooth = label\n",
    "            with ag.record():\n",
    "                outputs = [net(X) for X in data]\n",
    "                loss = [L(yhat, y) for yhat, y in zip(outputs, label_smooth)]\n",
    "            ag.backward(loss)\n",
    "            trainer.step(batch_size)\n",
    "            ######## init mod #######\n",
    "            if i==0 and epoch==0:\n",
    "                new_classifier_w = mx.nd.random_normal(shape=(1000, 512, 1, 1), scale=0.01)\n",
    "                final_conv_layer_params = net.output[0].params\n",
    "                final_conv_layer_params.get('weight').set_data(new_classifier_w)\n",
    "            ########################\n",
    "            acc_top1.update(label, outputs)\n",
    "            acc_top5.update(label, outputs)\n",
    "            train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            if log_interval and not (i+1)%log_interval:\n",
    "                _, top1 = acc_top1.get()\n",
    "                _, top5 = acc_top5.get()\n",
    "                err_top1, err_top5 = (1-top1, 1-top5)\n",
    "                logging.info('Epoch[%d] Batch [%d]\\tSpeed: %f samples/sec\\ttop1-err=%f\\ttop5-err=%f'%(\n",
    "                             epoch, i, batch_size*log_interval/(time.time()-btic), err_top1, err_top5))\n",
    "                btic = time.time()\n",
    "\n",
    "        _, top1 = acc_top1.get()\n",
    "        _, top5 = acc_top5.get()\n",
    "        err_top1, err_top5 = (1-top1, 1-top5)\n",
    "        train_loss /= num_batch * batch_size\n",
    "\n",
    "        err_top1_val, err_top5_val = test(ctx, val_data)\n",
    "        train_history.update([err_top1, err_top5, err_top1_val, err_top5_val])\n",
    "        train_history.plot(['training-top1-err', 'validation-top1-err'],\n",
    "                           save_path='%s/%s_top1.png'%(plot_path, model_name))\n",
    "        train_history.plot(['training-top5-err', 'validation-top5-err'],\n",
    "                           save_path='%s/%s_top5.png'%(plot_path, model_name))\n",
    "\n",
    "        logging.info('[Epoch %d] training: err-top1=%f err-top5=%f loss=%f'%(epoch, err_top1, err_top5, train_loss))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        logging.info('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch, err_top1_val, err_top5_val))\n",
    "\n",
    "        if err_top1_val < best_val_score and epoch > 50:\n",
    "            best_val_score = err_top1_val\n",
    "            net.save_params('%s/%.4f-imagenet-%s-%d-best.params'%(save_dir, best_val_score, model_name, epoch))\n",
    "\n",
    "        if save_frequency and save_dir and (epoch + 1) % save_frequency == 0:\n",
    "            net.save_params('%s/imagenet-%s-%d.params'%(save_dir, model_name, epoch))\n",
    "\n",
    "    if save_frequency and save_dir:\n",
    "        net.save_params('%s/imagenet-%s-%d.params'%(save_dir, model_name, epochs-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "* Run the cell below to start training\n",
    "* Logs are displayed in the cell output\n",
    "* An example run of 1 epoch is shown here\n",
    "* Once training completes, the symbols and params files are saved in the root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [99]\tSpeed: 453.202701 samples/sec\ttop1-err=0.998965\ttop5-err=0.994883\n",
      "INFO:root:Epoch[0] Batch [199]\tSpeed: 718.410684 samples/sec\ttop1-err=0.998945\ttop5-err=0.994902\n",
      "INFO:root:Epoch[0] Batch [299]\tSpeed: 811.818516 samples/sec\ttop1-err=0.998932\ttop5-err=0.994987\n",
      "INFO:root:Epoch[0] Batch [399]\tSpeed: 904.030753 samples/sec\ttop1-err=0.998892\ttop5-err=0.994844\n",
      "INFO:root:Epoch[0] Batch [499]\tSpeed: 1055.685132 samples/sec\ttop1-err=0.998863\ttop5-err=0.994750\n",
      "INFO:root:Epoch[0] Batch [599]\tSpeed: 1116.572733 samples/sec\ttop1-err=0.998887\ttop5-err=0.994613\n",
      "INFO:root:Epoch[0] Batch [699]\tSpeed: 1396.636375 samples/sec\ttop1-err=0.998836\ttop5-err=0.994350\n",
      "INFO:root:Epoch[0] Batch [799]\tSpeed: 1460.976810 samples/sec\ttop1-err=0.998684\ttop5-err=0.993970\n",
      "INFO:root:Epoch[0] Batch [899]\tSpeed: 1635.999958 samples/sec\ttop1-err=0.998594\ttop5-err=0.993513\n",
      "INFO:root:Epoch[0] Batch [999]\tSpeed: 1645.687634 samples/sec\ttop1-err=0.998504\ttop5-err=0.993145\n",
      "INFO:root:Epoch[0] Batch [1099]\tSpeed: 1859.177707 samples/sec\ttop1-err=0.998452\ttop5-err=0.992853\n",
      "INFO:root:Epoch[0] Batch [1199]\tSpeed: 1674.900204 samples/sec\ttop1-err=0.998382\ttop5-err=0.992529\n",
      "INFO:root:Epoch[0] Batch [1299]\tSpeed: 1616.574919 samples/sec\ttop1-err=0.998331\ttop5-err=0.992239\n",
      "INFO:root:Epoch[0] Batch [1399]\tSpeed: 1839.109477 samples/sec\ttop1-err=0.998259\ttop5-err=0.991889\n",
      "INFO:root:Epoch[0] Batch [1499]\tSpeed: 1722.576137 samples/sec\ttop1-err=0.998124\ttop5-err=0.991366\n",
      "INFO:root:Epoch[0] Batch [1599]\tSpeed: 1587.906096 samples/sec\ttop1-err=0.997952\ttop5-err=0.990752\n",
      "INFO:root:Epoch[0] Batch [1699]\tSpeed: 1896.990392 samples/sec\ttop1-err=0.997809\ttop5-err=0.990159\n",
      "INFO:root:Epoch[0] Batch [1799]\tSpeed: 1660.405105 samples/sec\ttop1-err=0.997683\ttop5-err=0.989603\n",
      "INFO:root:Epoch[0] Batch [1899]\tSpeed: 1705.921983 samples/sec\ttop1-err=0.997527\ttop5-err=0.988893\n",
      "INFO:root:Epoch[0] Batch [1999]\tSpeed: 1732.389939 samples/sec\ttop1-err=0.997350\ttop5-err=0.988210\n",
      "INFO:root:Epoch[0] Batch [2099]\tSpeed: 1702.131050 samples/sec\ttop1-err=0.997190\ttop5-err=0.987610\n",
      "INFO:root:Epoch[0] Batch [2199]\tSpeed: 1772.783530 samples/sec\ttop1-err=0.997013\ttop5-err=0.986838\n",
      "INFO:root:Epoch[0] Batch [2299]\tSpeed: 1642.015925 samples/sec\ttop1-err=0.996822\ttop5-err=0.985996\n",
      "INFO:root:Epoch[0] Batch [2399]\tSpeed: 1539.734393 samples/sec\ttop1-err=0.996619\ttop5-err=0.985212\n",
      "INFO:root:Epoch[0] Batch [2499]\tSpeed: 2247.187727 samples/sec\ttop1-err=0.996396\ttop5-err=0.984302\n",
      "INFO:root:[Epoch 0] training: err-top1=0.996390 err-top5=0.984281 loss=6.786635\n",
      "INFO:root:[Epoch 0] time cost: 1029.767129\n",
      "INFO:root:[Epoch 0] validation: err-top1=0.990580 err-top5=0.959400\n"
     ]
    }
   ],
   "source": [
    "if mode == 'hybrid':\n",
    "    net.hybridize()\n",
    "train(num_epochs, context)\n",
    "net.export(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
